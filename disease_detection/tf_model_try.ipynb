{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/zaynap/.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/zaynap/.local/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: opencv-python in /home/zaynap/.local/lib/python3.10/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: filelock in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: fsspec in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/zaynap/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zaynap/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/zaynap/.local/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: numpy in /home/zaynap/.local/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ResNet9, self).__init__()\n",
    "        # Example convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Define the fully connected (fc) layer\n",
    "        # Assuming the input to fc is flattened from a 128x32x32 tensor\n",
    "        self.fc = nn.Linear(128 * 32 * 32, num_classes)  # Adjust dimensions based on your architecture\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Example to calculate dimensions\n",
    "dummy_input = torch.randn(1, 3, 32, 32)  # Replace with your input shape\n",
    "model = ResNet9(in_channels=3, num_classes=10)\n",
    "\n",
    "# Pass dummy input through convolutional layers only\n",
    "with torch.no_grad():\n",
    "    features = model.conv1(dummy_input)\n",
    "    features = model.conv2(features)\n",
    "    print(features.shape)  # Inspect the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 4\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = ResNet9(in_channels=3, num_classes=10)\n",
    "\n",
    "# Example input tensor\n",
    "input_tensor = torch.randn(1, 3, 32, 32)  # Adjust shape as needed\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    predicted_class = output.argmax(1).item()\n",
    "\n",
    "print(f'Predicted Class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet9(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res1): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=512, out_features=38, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_277423/3847535290.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('models/plant-disease-model-complete.pth', map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the full model\n",
    "model = torch.load('models/plant-disease-model-complete.pth', map_location='cpu')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(model)  # Inspect the loaded model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match model's input size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')  # Load image\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet9' object has no attribute 'fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Perform inference\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m, in \u001b[0;36mResNet9.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m(x)  \u001b[38;5;66;03m# Fully connected layer\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet9' object has no attribute 'fc'"
     ]
    }
   ],
   "source": [
    "# Load and preprocess an image\n",
    "image_path = 'images_for_test/5c1b9970-0fc5-4aaf-8ae4-3255aa993630.jpeg'  # Replace with your image path\n",
    "input_tensor = preprocess_image(image_path)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    predicted_class = output.argmax(1).item()\n",
    "\n",
    "print(f'Predicted Class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during prediction: mat1 and mat2 shapes cannot be multiplied (1x25088 and 512x38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_286924/3346857973.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"models/plant-disease-model-complete.pth\", map_location=device)  # Load the entire model object\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Define the ResNet9 model or import it if defined in another file\n",
    "class ResNet9(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # AdaptiveAvgPool2d ensures that spatial dimensions are always (1, 1)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(128, num_classes)  # Match the output size of the conv layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.classifier(x)\n",
    "        print(\"the classifier output :\" , x.shape)\n",
    "        return x\n",
    "\n",
    "# Reload the model with the updated definition\n",
    "num_classes = 5  # Update based on the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet9(num_classes=num_classes)\n",
    "model = torch.load(\"models/plant-disease-model-complete.pth\", map_location=device)  # Load the entire model object\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a more reasonable size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(image):\n",
    "    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "    return transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# Predict for a single image\n",
    "def predict_image(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found at {image_path}\")\n",
    "\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(preprocessed_image)\n",
    "            if isinstance(predictions, torch.Tensor):\n",
    "                predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "            else:\n",
    "                raise ValueError(\"Model output is not a tensor. Check model implementation.\")\n",
    "\n",
    "        return predicted_class, predictions\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage\n",
    "image_path = \"images_for_test/cfb73761-e9b0-49de-bfa0-c24fa980f5b2.jpeg\"\n",
    "\n",
    "# Predict on an image\n",
    "predicted_class, predictions = predict_image(image_path)\n",
    "if predicted_class is not None:\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input size: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected input size:\", model.conv1[0].in_channels)  # إذا كنت تستخدم Conv2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during prediction: a Tensor with 3136 elements cannot be converted to Scalar\n"
     ]
    }
   ],
   "source": [
    "predicted_class, predictions = predict_image(image_path)\n",
    "if predicted_class is not None:\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated image saved to annotated_image.jpg\n",
      "Predicted Class: Tomato___healthy\n",
      "Predictions: tensor([[ -77.8094,   -0.1820, -108.9794, -131.8778, -104.7580, -111.1996,\n",
      "          -95.5578, -109.2154,  -50.7834, -155.1822, -107.0426,  -88.8749,\n",
      "          -89.5843, -114.3271, -104.8174, -171.8780,  -81.2308,  -48.2699,\n",
      "          -13.6952,  -12.1107,  -98.8678, -161.6070, -117.0350,  -90.4716,\n",
      "         -130.2487, -168.7204, -110.2874, -126.8831, -129.9841,  -31.6615,\n",
      "          -31.9136, -108.0810,  -42.1047, -136.2311,  -70.8965,  -89.1100,\n",
      "          -35.6075,  -96.8336]], device='cuda:0')\n",
      "Annotated Image Path: annotated_image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_286924/519424502.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"models/plant-disease-model-complete.pth\", map_location=device)  # Load the entire model object\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        if pool:\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels, num_diseases):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(in_channels, 64)\n",
    "        self.conv2 = ConvBlock(64, 128, pool=True)  # out_dim: 128 x 64 x 64\n",
    "        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128))\n",
    "        self.conv3 = ConvBlock(128, 256, pool=True)  # out_dim: 256 x 16 x 16\n",
    "        self.conv4 = ConvBlock(256, 512, pool=True)  # out_dim: 512 x 4 x 4\n",
    "        self.res2 = nn.Sequential(ConvBlock(512, 512), ConvBlock(512, 512))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_diseases)\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Reload the model with the updated definition\n",
    "num_classes = 38   # Update based on the dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet9(in_channels=3,num_diseases=num_classes)\n",
    "model = torch.load(\"models/plant-disease-model-complete.pth\", map_location=device)  # Load the entire model object\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to a more reasonable size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(image):\n",
    "    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB\n",
    "    return transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "class_labels = ['Tomato___Late_blight', 'Tomato___healthy', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Potato___healthy', 'Corn_(maize)___Northern_Leaf_Blight', 'Tomato___Early_blight', 'Tomato___Septoria_leaf_spot', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Strawberry___Leaf_scorch', 'Peach___healthy', 'Apple___Apple_scab', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Bacterial_spot', 'Apple___Black_rot', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Peach___Bacterial_spot', 'Apple___Cedar_apple_rust', 'Tomato___Target_Spot', 'Pepper,_bell___healthy', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Potato___Late_blight', 'Tomato___Tomato_mosaic_virus', 'Strawberry___healthy', 'Apple___healthy', 'Grape___Black_rot', 'Potato___Early_blight', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Common_rust_', 'Grape___Esca_(Black_Measles)', 'Raspberry___healthy', 'Tomato___Leaf_Mold', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Pepper,_bell___Bacterial_spot', 'Corn_(maize)___healthy']  # Replace with your actual labels\n",
    "\n",
    "# Function to annotate the image with the predicted class\n",
    "def annotate_image(image_path, predicted_class):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image not found at {image_path}\")\n",
    "    \n",
    "    # Add a rectangle (bounding box) - Adjust coordinates as needed\n",
    "    height, width, _ = image.shape\n",
    "    start_point = (10, 10)  # Top-left corner of the bounding box\n",
    "    end_point = (width - 10, height - 10)  # Bottom-right corner of the bounding box\n",
    "    color = (0, 255, 0)  # Green color for the bounding box\n",
    "    thickness = 2  # Thickness of the rectangle\n",
    "    \n",
    "    cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "    \n",
    "    # Add the predicted label text\n",
    "    label = f\"Predicted: {class_labels[predicted_class]}\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    text_color = (0, 255, 0)\n",
    "    text_thickness = 2\n",
    "    \n",
    "    cv2.putText(image, label, (20, 50), font, font_scale, text_color, text_thickness)\n",
    "    \n",
    "    # Save the annotated image\n",
    "    annotated_path = \"annotated_image.jpg\"\n",
    "    cv2.imwrite(annotated_path, image)\n",
    "    print(f\"Annotated image saved to {annotated_path}\")\n",
    "    \n",
    "    return annotated_path\n",
    "\n",
    "# Predict for a single image\n",
    "def predict_image(image_path):\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found at {image_path}\")\n",
    "\n",
    "        preprocessed_image = preprocess_image(image)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(preprocessed_image)\n",
    "            if isinstance(predictions, torch.Tensor):\n",
    "                predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "            else:\n",
    "                raise ValueError(\"Model output is not a tensor. Check model implementation.\")\n",
    "\n",
    "        # Annotate the image\n",
    "        annotated_image_path = annotate_image(image_path, predicted_class)\n",
    "        \n",
    "        return predicted_class, predictions, annotated_image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Example usage\n",
    "image_path = \"images_for_test/Lavender-with-White-Spots.png\"\n",
    "\n",
    "# Predict and annotate the image\n",
    "predicted_class, predictions, annotated_image_path = predict_image(image_path)\n",
    "if predicted_class is not None:\n",
    "    print(f\"Predicted Class: {class_labels[predicted_class]}\")  # Display the class label\n",
    "    print(f\"Predictions: {predictions}\")  # Display the raw predictions\n",
    "    print(f\"Annotated Image Path: {annotated_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
